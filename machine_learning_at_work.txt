<Machine learning at work 머신러닝 실무 프로젝트 - 아리가 미치아키 - 한빛미디어>

* 모델:
  - 인풋데이터, 아웃풋 데이터 사이에 드러나지 않은 관계를
  수식, 규칙으로 근사한것
  - 알고리즘과 파라미터로 구성됨

* 인풋데이터 : 주로 벡터화(여러가지 데이터를 한 배열에 넣은 모양)

* 머신러닝 프로젝트 과정 (대체로):
  1. 문제 정의 - 목적이 무엇인지 명확히!
  2. 머신러닝을 사용하지 않는 방법 없는지 검토!!! - 기술부채(출시를 서두르기 위해 해결하지 못한 문제, 어설픈 설계, 문서화, 컴파일러 경고 등) 쉽게 누적 때문
    : 해결하려는 문제를 머신러닝 문제로 바꾸기

  3. 머신러닝 시스템을 설계
  4. 사용할 알고리즘 선택
  5. 특징과 정답 데이터, 로그를 설계 ???
  6. 데이터 수집과 전처리
    : 문제 풀기 위한 도구 선택 및 전처리

  7. 학습 수행, 파라미터 튜닝
    : 모델 구축

  8. 시스템에 통합
    : 기존 서비스에 통합

* 해결하려는 문제 자체가 사람도 하기 어려운 일이라면 머신러닝으로 해결하는 것 역시 어렵다
  (특히 지도학습, 정답이 무엇인지 사람이 기계에게 알려줘야함)


* 머신러닝 문제 해결 사례 핵심
  - 알고리즘: 어떤 알고리즘 사용?
  - 데이터특징: 어떤 데이터를 특징으로 사용?
  - 통합: 머신러닝 부분 어떻게 통합?

* 머신러닝으로 해결 가능한가? 아닌가? 구분!
  모델 구현하는 일(데이터 분석의 80%는 전처리)
  머신러닝 시스템 개발은 시행착오를 반복하는 과정
  4~7번 반복, 그러므로 시행착오 효율적으로 하는 것 중요



=========================================
<Chapter 1 : 머신러닝 프로젝트 시작하기>

<머신러닝 시스템 프로젝트 과정>

@1. 문제 정의 - 목적이 무엇인지 명확히!

@2. 머신러닝을 사용하지 않는 방법 없는지 검토!!!

    - 기술부채(출시를 서두르기 위해 해결하지 못한 문제, 어설픈 설계, 문서화, 컴파일러 경고 등) 쉽게 누적 때문

    오랜 운용 > 사용자 경향 변함 > 입력 경향 바뀜 (데이터 옥동자라는 의미 변화 예시)
    그러므로 데이터 모델도 갱신(유지보수 필요)

    애초에 머신러닝은 대량 데이터 처리에 사용되므로
    생각하지 못한 예측 결과 출력 위험 항상 존재
    (구글 포토 : 흑인을 고릴라로 인식 > 인종차별 문제 사례)

    * 2가지 조건 만족 할때 머신러닝 적용 하면 좋음
      1. 대량의 데이터에 대해 고속으로 안정된 판단 내려야한다
      2. 예측 결과에 일정 수준 오류가 용인 된다 : 예측이 100% 정확하지 않음 > 운영 쪽에서 이 오류 대응할 구조 꼭 필요
        (의도치 않은 예측결과 나오면 사후 개입하는: 고릴라같은 아웃풋은 삭제하는 룰베이스 예외처리!)

    * 최소기능제품 만들기 (MVP : minimum viable product) == 가설 검증
      : 처음 세운 가설 맞는지 미리 검증! (모델 구현 테스트 후 목표자체가 틀릴수도 있으니까)


@3. 머신러닝 시스템을 설계

    * 핵심
    1. 예측결과 어떻게 이용 : 배치 후 디비저장(따로 새벽에 모델 돌려서 예측값 얻어놓기) /혹은/ 실시간 비동기로 활용(유저 액션에 따라 바로바로 예측하기)
    2. 예측 오류의 영향을 어떻게 흡수 : 예측후 사람이 사후 체크 프로세스 만들기! /혹은/ 잘못된예측이 큰 영향 없을때 사용

    * 구체적인 목표 성능과 포기 지점 설정하기
    - 없으면 전처리 하면서 데이터 도메인 지식 생겨서... 근거없는 자신감으로 성능 계속 더 개선할 수있다는 착각의 늪 빠짐 > 매몰비용 높아짐
    (2개월 안에 90% 성능! 과같이 구체적으로 계획하기)


@4. 사용할 알고리즘 선택

    - 데이터 특성을 아느냐 : 정답을 안다면(지도학습) : 모른다면 군집화같은(비지도학습) 혹은 데이터 시각화를 통해 특징 알아내기
    - 데이터 양 : 온라인 학습 /혹은/ 배치 학습

@5. 특징과 정답 데이터, (로그를 설계: 어플리케이션 case)

    - 피쳐(특징): 어떤 피쳐를 사용할 것인가? : 도메인 전문가 협조 필요
    - 범주형 변수를 > dummy더미 변수로(원핫인코딩, 0,1수치데이터화)
    - numeric(수치형)변수는 그대로 사용 가능

    고전 머신러닝은 어떤 피쳐 선택하느냐가 핵심이었음 > 딥러닝에선 피쳐보단 신경망 구조가 핵심

    - 데이터 레이블링(정답지 만들기)
    - 웹 어플리케이션의 로그로 정답 데이터 만듬 (어떤 페이지 클릭! 광고 효과 예시)
    (로그 설계시 생각나는 특징 모두 포함 시켜야함! 로그 수집 한번 시작하면 바꾸기 어려움, 바꿔도 그전 데이터 사용 못하게됨)


@6. 데이터 수집과 전처리
    - 정상범주 넘어간 이상값 처리
    - 값의 변동폭으로 인한 영향 줄이는 정규화
    - 범주형데이터 > 수치형데이터로 변환
    등등


@7. 학습 수행, 파라미터 튜닝
    - 하이퍼 파라미터 조정하며 > 시행착오 거치기 > 더 나은 성능 찾기 (예측한 성능 기준치를 넘는 것을 목표로 하기! 목표없는성능개선X!)

    *(처음 작성한 코드가 오류없이 작동할때 느끼는 불안감 == 첫번째 예측모델에서 99% 성능이 나올때): 뭔가 의심해야함
    - 과적합 (학습데이터에만 적합, 새 데이터는 예측 못함) <=> 일반화 generalization(처음보는 데이터 처리 잘함)
    - 학습데이터에 테스트데이터 섞이는 (데이터 유출 data leakage)
    일 수 있음

    * 오답낸 예측 결과를 살펴봄 > 원인 무엇인지? 오답 공통점 없는지?
    (만약, 결과 계속 안좋다면 4단계 알고리즘 검토부터 다시)

    * 과적합 방지
    1. 교차검증 cross validation : 학습데이터를 training훈련, validation검증 (9:1)로 나눠서
      검증데이터로 성능을 그때그때 측정해가며 학습시키기

    (- 학습데이터 = (트레이닝데이터 + 검증데이터)
    - 총 데이터 = 학습데이터 + 테스트데이터)

    2. 규제화 regularization : 약간 성능 낮춰서 더 일반화 시키기 (드랍아웃처럼)
    3. 학습곡선 살피기 : learning curve



@8. 시스템에 통합
    - 예측 성능이 > 비즈니스 성과(목표) 달성하는 지 여부 모니터링
    - 데이터 경향 변함 > 그러므로 모델도 꾸준히 개선 필요 (시스템 통합하면 끝 X)
====================
<머신러닝 시스템 문제 대처>

* 머신러닝 시스템 운영 문제
A. 확률적인 부분 있어서 자동 테스트 어려움
B. 오래 운용하면 사용자 경향 변해서 입력 경향도 변함
C. 처리 파이프라인이 복잡해짐
D. 데이터 의존 관계가 복잡헤짐
E. 실험 코드 혹은 파라미터가 포함되기 쉬움
F. 개발 시스템과 운영 시스템 간의 언어/프레임워크가 제각기이기 쉽다

* 문제 대처법
1. 황금기준을 사용한 예측 성능 모니터링 (A, B, D) : 예측 성능에 임계값 알림 설정하기(일정성능 안나오면 알람)
2. 예측 모델 모듈화, 모델에 대한 A/B 테스트 실시 (B) : 예측 모델 병렬로 두고 바꿔가며 성능 테스트
3. 모델 버전관리 > 원하는 모델로 돌아가기 가능하도록 (D, E) : 자유로운 롤백 (소스코드, 모델, 데이터 모두 버전관리 하기)
4. 데이터 처리 파이프라인 자체를 저장 (C, E) : 전처리 과정 코드도 모델과 함께 저장하기 (자연어처리시 단어 분절, 단어 빈도 측정의 전처리 단계에서도 파라미터튜닝이 존재하기 때문)
5. 개발 시스템과 운영 시스템 간의 언어/프레임워크 일치시키기 (F) : 파이썬 텐서플로우 모델 + 파이썬 장고 웹 프레임워크, 그래야 마이그레이션과 커뮤니케이션 비용 줄임 /혹은/ REST API서버화(마이크로 서비스)

* 딥러닝 부상 전의 머신러닝 사실상 표준은 : 사이킷런

=================

<머신러닝 시스템 성공적 운영>

* 일반 시스템 개발과는 달리
머신러닝 시스템은 개발해도 결과물이 안나올수도 있음!!!
(ㅋㅋㅋ 삼성해커톤 소음예측모델개발 경험 ㅋㅋㅋ 어려우면 다른 방식으로라도 만드는 웹어플개발과 다름... 모델 학습이 안되면 쓸수가 없고 다른방법이 없음)

* 키 리소스(인력)
  1. 도메인 전문가 (도메인)
  2. 통계(수학), 머신러닝 전문가 (데이터 사이언티스트) : 문제 설정하는 의사소통 능력 필수
  3. 데이터 분석 인프라 구축 가능한 엔지니어 (컴퓨터 사이언스)
  4. 실패에 대한 위험을 짊어질 수 있는 책임자 : 머신러닝 프로젝트는 실패 확률이 높은 투자임! 머신러닝으로 가치를 얻기를 설득할수 있는 사람! 실무자가 실무에 집중할수있게!
  (4번은 처음 알았다!!!)


===================
<정리 >
< 머신러닝 프로젝트 진행할때는 >
1. MVP로 개념 검증 최우선
2. 머신러닝 이외 해결 방법 꼭 찾아보기!!!
3. 머신러닝에 적합한 문제인지 파악
4. 예측 성능 과 비즈니스 성과!!! 모두 충족시키는지 모니터링


===================
내 머신러인 프로젝트 실패 경험이 정말 값진거였다는걸 깨달았다
1. 후후앤컴퍼니 스팸지수예측 프로젝트와
2. 삼성AI해커톤 소음예측 노이즈캔슬링 프로젝트
모두 머신러닝 만능설 같은 내 고정관념을 깨주었고
웹어플 개발과는 다르게 결과물이 안나올수도 있다는 걸 알려줬고
무엇보다 성능이 먼저 검증이 안되면 시작하면 안된다는 교훈을 주었다
머신러닝 시스템 자체가 실패 가능성이 높은 시스템이라는 사실!!!
===================


=========================================
<Chapter 2 : 머신러닝으로 해결가능한 문제>

<머신러닝 알고리즘 종류>
1. 분류 : 정답이 카테고리
2. 회귀 : 정답이 수치
3. 군집화 : 데이터 그룹화
4. 차원 축소 : 시각화 /혹은/ 계산량 절감
5. 그 외 :
  - 추천
  - 이상 탐지 : 평소와 다른 행동 검출
  - 고빈도 패턴 마이닝 : 발생 빈도 높은 패턴 추출 (기저기와 맥주 사례)
  - 강화학습 : 정답 불명확 환경에서 취할 행동 선택


* 알고리즘 선택 프로세스
데이터가 충분한가(수집) > 범주를 예측하는가(수치 예측) > 정답에 레이블이 있는가(없는가) >

==================지도학습===================
* 분류:
  - 퍼셉트론
  - 로지스틱 회귀
  - 서포트 벡터 머신(SVM)
  - 신경망
  - K-NN(최근접이웃)
  - 결정트리
  - 랜덤포레스트
  - 경사부스팅 결정트리(GBDT)

  그외)
  - 나이브 베이즈 Naive Bayes : 텍스트분류에 사용
  - HMM 히든 마코프 모델 : 음성인식에서 사용

<참고 사항>
  *온라인 학습 : 최적화 하는 방식이 데이터를 하나씩 입력 : 예) 퍼셉트론
   배치 학습  : 최적화 하는 방식이 데이터를 한꺼번에 입력

  *선형분리 (linearly separable) : 무조건 직선으로 나눠짐(직선이 아니라 곡선이라면 X)

  *계단함수step function : 옛날에 쓰던 활성함수 (입력값을 +1 또는 -1로만 바꿔줌)

<퍼셉트론 특성>
    온라인 학습방식
    예측 성능 보통, 학습 빠름
    과적합되기 쉬움
    선형분리 가능한 문제만 풀수있음

    hinge loss 힌지 손실 사용 (렐루 반대모양 )
    SGD stochastic gradient descent 확률적경사하강법 을 사용해 W가중치백터(파라미터) 최적화 함
    활성함수로 계단함수 사용

<로지스틱 회귀>
    출력뿐만 아니라 출력값에 해당하는 속할 '확률'을 계산 가능
    온라인 학습, 배치 학습 모두 가능
    예측 성능 보통, 학습 속도 빠름
    과적합 방지하는 규제항 추가되어있음
    결정경계가 직선이기 때문에 선형 분리 가능한 문제만 풀수있음

    활성함수로 sigmoid, cross-entropy error function 사용

<서포트벡터머신>
    선형분리가 불가능하는 문제에도 적용가능
    마진 최대화를 통해 매끈한 초평면을 학습할 수 있다
    커널이라는 방법을 이용하여 비선형 데이터를 분리할수있다
    선형 커널로는 차원수가 높은 희소sparse 데이터도 학습할수있다
    배치 학습과 온라인 학습에 모두 적용할 수 있다

    * 서포트벡터 : 초평면으로부터(두 집단 경계) 가장 가까운 각 클래스의 데이터
    힌지 손실 가로축 교점이 다름 (0이 아니고 y = 1이 교점)
    이것 때문에 결정경계에 아슬아슬하게 걸처 정답이 된 데이터도 약하게 패널티가 부가되어 결정경계에 마진이 생김
    마진 최대화 생김

    * 두가지 특성
    이렇게 마진 최대화가 : 과적합 억제효과 생김
    커널 기법 : 선형분리가 불가능한 데이터 차원수를 늘려서 선형분리가 가능하게 만듬

    선형커널, 다항식 커널, RBF커널, 등 존재

<신경망>
    다층퍼셉트론

    비선형 데이터를 분리 가능(퍼셉트론과 차이)
    학습 시간이 오래 걸림(GPU기술 발전으로 해결)
    파라미터 수가 많으므로 과적합을 일으키기 쉬움
    가중치의 초기값에 민감하여, 로컬 최적점에 빠지기 쉬움

    softmax함수 사용
    활성함수로 ReLU 함수 사용
    학습에 backpropagation 사용

<k-NN>
    새로 입력된 데이터 > 가장 가까운 k개의 데이터 선택 > k개의 데이터의 다수결 결정(3개 선택중 AAB 라면 A가 2개로 더 많으니까 새 데이터도 A로 판별)

    데이터를 하나씩 순차적으로 학습
    기본적으로 모든 데이터와의 거리를 계산해야 하므로 예측 시간이 걸린다
    k값에 따라 편차는 있지만 예측 성능은 쵄찮은 편이다
    정규화를 통해 피처들끼리의 스케일을 맞출필요가 있다(피처 (예:x,y)축마다 스케일 차이가 있으면 학습이 제대로 되지 않음)

    거리는 유클리드 거리, 마할라노비스 거리를 사용하기도 한다

<결정트리, 랜덤포레스트, GBDT>
    (특성)
    학습한 모델을 사람이 해석하기 쉽다(시각화하여)
    입력 데이터를 정규화할 필요가 없다
    범주형 변수나 데이터의 누락값(계측 오류 등으로 값이 존재하지 않는 경우)이 있어도 용인된다.
    특정 조건이 맞으면 과적합을 일으키는 경향이 있다. (트리가 깊어질수록 학습에 사용되는 데이터 수가 적어져서 과적합 일어나기 쉬움 >> pruning 가지치기로 어느정도 방지 가능)
    비선형 문제에는 적용할 수 있지만, 선형 분리 문제는 잘 풀지 못한다.
    데이터 분포가 특정 클래스로 쏠려 있으면 잘 풀지 못한다
    데이터의 작은 변화에도 결과가 크게 바뀌기 쉽다.
    예측 성능은 보통이다.
    배치 학습으로만 학습할 수 있다.
    학습 결과가 IF-THEN 형태의 규칙이 됨

    피쳐가 많아도 과적합을 일으키기 쉬우므로 미리 차원 축소하거나 특징 선별해줄것

    결정트리 응용 기법 (앙상블 기법 : 여러 학습 결과를 조합하는 기법)
    - 랜덤포레스트 : 결정트리를 위한 피쳐들 조합 몇가지 마련 > 성능 좋았던 몇개의 학습기가 내린 예측 결과를 다수결로 통합
              각각의 트리가 독립적으로 학습 = 학습과정 병렬화
    - GBDT : 표본추출한 데이터를 이용해 순차적으로 얕은 트리를 학습해가는(직렬화) 알고리즘
            예측값과 실제값의 오차를 목표변수로 삼는 방법으로 약점 보안
            학습이 순차적으로 이루어지기때문에 랜덤포레스트보다 학습 시간 오래걸림고 파라미터수 많음
            (ex: XGBoost, LightGBM)

* 회귀 : 지도학습의 한 갈래, 연속된 값(수치) 예측
    - 선형 회귀, : 데이터를 직선으로 근사
    - 다항식 회귀 : 데이터를 곡선으로 근사
    - 라쏘 회귀, : L1규제(학습가중치 절대값)를 규제항으로 사용
    - 릿지 회귀, : L2규제(학습가중치 제곱)를 규제항으로 사용
    - 일래스틱넷 : 선형 회귀에 두가지 규제항 추가
    - 회귀 트리 : 결정트리에에 기초한 회귀 기법 (비선형 데이터 근사 가능)
    - SVR (support vector regression) : SVM에 기초한 회귀 기법 (비선형 데이터 근사 가능)

<선형회귀 원리>
  퍼셉트론 구조에서 활성함수 부분을 없앤것 = (수치를 바로 출력)

==================비지도학습===================
* 군집화
  데이터의 경향성 파악하는데 사용하는 기법
  - 계층적 군집화 hierarchical clustering
  - k-means : k개의 그룹으로 나누는 기법

* 차원축소 dimension reduction
  정보를 가능한 온전히 보존하면서 고차원 데이터를 저차원 데이터로 변환하는 기법
  (ex: 100차원인 데이터를 2차원으로 축소 >> 시각화 >> 데이터 경향 파악)

  차원축소 주요 기법
  - 주성분 분석(PCA : principle component analysis)
  - t-SNE

* 이상 탐지 : 이상값은 보통 데이터 분포가 매우 치우친 클래스이기 때문에,
          지도학습을 하게되면 이상값의 특징을 미처 학습하지도 못하고
          모든값에 정상이라고 분류해도 정확도가 99%가 될수있음 (imbalanced data 불균형 데이터 문제)
          그래서 이상탐지에는 비지도학습기법을 주로 사용함
          (사이킷런의 One Class SVM 사용 가능)

* 패턴 마이닝 : 데이터에서 자주 발견되는 패턴 추출 기법
          기법
          - association rule 연관법칙: a priori 알고리즘 사용
          - ARIMA(자기회귀 이동평균 모델)알고리즘 : 시계열 분석에 주로 사용

* 강화학습 : 정답 불명확 환경에서 목적을 위해 취할 행동 선택, 시행착오 거치며, 승리에 도움되면 보상, 그렇지 않으면 패널티 부가하는 식으로 행동정책 학습

============================================

=========================================
<Chapter 3 : 학습 결과 평가하기>
<평가하기>
* 평가 4가지 지표
  - 정확도 accuracy
  - 정밀도 precision
  - 재현율 recall
  - F-점수 F-measure

* confusion matrix
  TP : true positive
  TN : true negative
  FP : false positive
  FN : false negative

              |   예측결과  |
              | 양성 | 음성 |
  | 실제 | 양성 |  TP   FN
  | 결과 | 음성 |  FP   TN


  <정확도 accuracy>
    (TP + TN) / (TP + FP + TN + FN) : 정답과 일치한 수 / 전체 데이터 수
  <정밀도 precision>
    TP / (TP + FP) : 모든 output_true 중 정답맞춘_true 개수
  <재현율 recall>
    TP / (TP + FN) : 모든 정답_true 중 정답맞춘_true
  <F-점수 F-measure>
    정밀도와 재현율의 조화평균 : 점수가 높을수록 재현율과 정밀도가 고르게 높다는 의미

* 다중 클래스 분류의 평가 평균 구하기
    만약 세개의 클래스 분류라면 TP1 FP1 TP2 FP2 TP3 FP3 이 생김
  - 정밀도 마이크로 평균 : (TP1 + TP2 + TP3) / (TP1 + TP2 + TP3 + FP1 + FP2 + FP3)
  - 정밀도 매크로 평균 : (정밀도1 + 정밀도2 + 정밀도3) / 3
    (매크로 평균은 전체 성능 양상을 알기에 적합)

* 모델 성능 비교시, 데이터 분포 차이가 있는 경우 주로 F-점수 사용
  실제 문제는 정밀도와 재현율 중 무엇을 중시할지 고려하여 최소한의 성능을 보장하는 방향으로 튜닝 할것

  ex: '정밀도가 0.9미만인 모델은 채택하지 않는다'라면 > 정밀도를 먼저 0.9로 최소 조건 만족시키고 > 그 조건 만족하면서 F-점수가 높아지도록 파라미터튜닝

* 그 외에
  - ROC곡선
  - AUC 등을 평가 지표로 쓰기도 함

* !!! 모델성능평가는 비즈니스에 적용할 품질 최소 기준이라 생각하기
  모델 성능 튜닝에 너무 몰두하다보면 그 자체가 목적이 되기 쉬움!!!
  < 학습 모델 성능 != 비즈니스 목적 달성 > 별개문제임
  모델을 사용하는 목적이 무엇인지 항상 상기하기!

============= 그 외 평가법 =================
* 회귀 모델 평가
  - RMSE root mean squared error
  - 결정 계수 coefficient of determination

* A/B 테스트
  다양한 방식의 모델 프로토타입을 만든뒤, 동시에 사용자에게 사용하게 하여 더 좋은 평가를 받은 것을 채택하는 것
  (ex: 같은 웹사이트에 버튼 색깔, 버튼 문구 를 다르게 랜덤으로 사람들에 보여준뒤 어떤 색깔, 어떤 문구일때 더 반응이 좋았는지 보는 것)


=========================================
<Chapter 4 : 기존 시스템에 머신러닝 통합하기>

* 배치학습 <=> 온라인학습
  배치 batch 학습 = 일괄학습 (많은 데이터를 한번에 처리)
  온라인학습 = 순차학습 = 실시간학습 (데이터를 1건씩 순서대로 처리)

  결국 차이는
  학습 도중 최적화하는 방법 차이,
  한번에 다루는 데이터 덩어리 크기의 차이

  * 미니배치학습 mini-batch training : 일괄 학습과 순차 학습의 절충, 일정규모 데이터 샘플링하여 그룹 만들고, 그룹에 속하는 데이터를 일괄 학습 수행 > 이것을 반복
    - 예) 일반적으로 텐서플로우 모델 학습시킬때, 한번에 그 수많은 트레이닝 데이터를 다 읽지 못하니까
    데이터 300만개중, 100만개씩 나눠서 3번 순차적으로 학습시키는 것을 의미하는 듯함
    그렇게 총 300만개 데이터를 한 바퀴 돌아야 1에포크 되는 것?

* 어플리케이션과 통합 설계 방식
  1- 배치처리로 학습 + 예측결과를 어플리케이션에서 직접 산출(예측을 실시간 처리)
  2- 배치처리로 학습 + 예측결과를 API를 통해 사용(예측을 실시간 처리)
  3- 배치처리로 학습 + 예측결과를 DB에 저장하고 사용(예측을 배치 처리)
  4- 실시간 처리로 학습

  보통 모델을 학습시켜서 W가중치 값들을 저장해놓고(배치로 미리 학습해놓은 모델 확보)
  그 모델 기능만 응답해주는 API 서버를 따로 만들어서
  웹어플리케이션에서 필요할때만 API 호출보내는게 2번 방식

  1번은 어플리케이션 안에 내부적으로 모델을 넣어놓는것

  3번은 유저가 잘 쓰지 않는 새벽시간대에 한번씩 데이터로 모델을 학습시키고 미리 예측값들을 따로 DB에 저장해 놓은뒤
  어플리케이션이 필요할때 DB에서 그 값을 꺼내 사용하는것

* 머신러닝 언어와 어플리케이션 프레임워크 언어를 일치시키는게 좋지만
  어플리케이션은 Node.js, 모델은 python이라면
  모델을 임포트/익스포트할 수 있야함
  아님 Node.js로 모델을 직접 구현하든지...(머신러닝 라이브러리는 디버깅이 어려운 프로그램이라 구현 힘듬)

<로그 설계>

* 데이터 정보는 크게 3개로 나뉨
  1. 사용자 정보 : 성별, 나이 등등(서비스 가입자 정보)
  2. 콘텐츠 정보 : 상품 범주, 상품 세부 정보, 가격, 색깔 등
  3. 사용자 행동 로그 : 어떤 페이지에 접속했는지, 구매 이벤트 발생 등 (광고 클릭이 어떻게 구매로 이어졌는지)

* 1,2번 사용자정보와 콘텐츠정보는 RDBMS에 저장
  3번 사용자행동로그는 데이터양이 많기 때문에 분산RDBMS, 하둡파일시스템 등에 저장

  로그데이터는
  - 분산 RDBMS 혹은 하둡 분산파일시스템 혹은 객체 저장소에 저장(데이터 양이 많기 떄문에)
  - SQL 질의가 가능하도록 해놓을것(필요 데이터 정보를 뽑아내고 전달하는 과정이 쉬워지도록)

  * 대규모 데이터를 전송하는 비용 고려하기
  대규모 데이터를 사용하는 머신러닝에서 가장 큰 병목은 데이터 전송시간임
  가능한 데이터를 로컬 머신으로 내려받지 않을 수단 마련해야함
  데이터 전처리를 가능한 분산 RDBMS에서 SQL로 수행하는 것이 바람직


* 로그 설계시 주의사항
  - 로그에는 가능한 다양한 정보를 포함하는 것이 좋다. (후에 추가히가 힘듬)
    성별 ,방문시간대, 광고 유형 등등

  - 훈련 데이터로 만들수있는지 미리 살피기
    광고 표출 로그가 너무 많다는 이유로 저장안하고
    광고 클릭 로그만 수집하면
    표출된 광고가 클릭되지 않은 건은 알수없어서
    클릭 예측 훈련데이터 못 만듬

  - 데이터 변경 이력 저장해두기
    콘텐츠 설명문과 매상의 관계 파악에서
    만약 콘텐츠 설명문을 변경한적이 있다면
    어떤 설명문을 어떤 기간동안 사용했는지 알아야 충분히 조사 가능

  - 로그 포맷 변경도 주의하기



=========================================
<Chapter 5 : 학습 데이터 수집하기>

* feature 찾기 : 보통 heuristic 방법으로 찾음 (발견법)
  - 서비스 도중 발생하는 로그를 사용 (완전 자동)
  - 콘텐츠를 사람이 직접 보며 부여 (수동)
  - 기게적으로 정보를 부여한 뒤 사람이 확인 (자동 + 수동)

* 방법들
  1. 공개된 데이터 셋이나 모델 활용
  2. 개발자가 직접 데이터 만듬
  3. 동료나 친구에게 데이터 입력 부탁
  4. 크라우드소싱 활용
  5. 서비스(어플리케이션)에 수집기능을 넣고, 유저 입력을 활용


<1. 웹상에 공개된 데이터 셋이나 모델 활용>
    주의할점 :
    - 모델이나 데이터셋의 라이선스가 상업적 이용을 허용하는가?
    - 원하는 분야에 적용 가능 한가? : 바로 사용 못하고 가공해야할 수도 있음 (반지도학습 semi-supervised learning, 전이학습 transfer learning)

<2. 개발자가 직접 데이터 만듬>
    * 특정문제가 사람이 직접 풀기 힘들면 머신도 풀기 힘듬
    카테고리 분류가 애매한 데이터가 있다면, 고민 (새 카테고리를 만드느냐, 두 카테고리에 모두 포함되게 만드느냐 등등)
    데이터 만들면서 생기는 인사이트 활용하기(ex: 기사제목에 포함된 단어로 카테고리 분류하면 효과적이다)
    분류기준이나 분류 어려운 콘텐츠는 따로 정리해두어야함
    혼자 데이터 만드는 작업하면 편견이 개입될수 있다 (여러명이 참여하는 식으로 보완)

<3. 동료나 친구에게 데이터 입력 부탁>
    중복작업 위험 주의(레이블링 어노테이션 툴, 협업 툴 사용하기)
    데이터 분류 기준 문서화하기 (데이터 포맷이나 레이블 방향성 일치하도록)

<4. 크라우드소싱 활용>
    데이터 만드는 작업을 마이크로태스크로 작고 단순하게 만들어, 불특정 다수 사람에게 부탁하는 것

    * 특징
    - 전문가를 고용하는 것보다 작업이 빠르고, 비용이 낮음
    - 작업속도가 빠르지만, 그만큼 시행착오 여러번 반복할 수도 있음
    - 비용이 낮으므로 여러 사람에게 같은 일을 맡겨 중복작업 가능성 있음

    * 주의할점
    - 작업자가 단시간에 끝낼 수 있어야 하므로 작업을 설계하기 까다로움
    - 높은 전문선이 요구되는 작업은 절차를 잘 세분화해서 자세히 설명해야함
    - 작업 결과의 품질을 높이려면 결과를 주의해서 가공해야함

    * 팁
    - 같은 작업을 여럿에게 맡긴뒤 중복 데이터 레이블의 최종안을 다수결로 최종 결정
    - 미리 연습문제를 풀게하거나 설문조사를 통해 작업자를 걸러내기
    - 데이터 품질 평가 방법 고려하기

<5. 서비스(어플리케이션)에 수집기능을 넣고, 유저 입력을 활용>
    크라우드소싱과 비슷

    * 예시
    - 유저에게 간단한 설문조사 실시
    - 유저가 콘텐츠에 태그를 부여할수 있게 기능 추가
    - 부적절한 서비스 결과를 신고할 수 있게 하는 기능 추가

    * 주의할점
    - 이용자 수 일정규모 이상이어야함
    - 참여자에게 줄 보상 설계해야함



=========================================
<Chapter 6 : 효과 검증하기>

의도한 효과가 맞는가? 어느정도 효과가 생겼는가?

검증 : 어떤 측정값에 기초한 판단
효과 검증 : 사건 Y가 X의 영향을 얼마나 받았는지 밝히는 활동

* 효과 검증 단계 : 문제정의 > 가설설정> 개입 > 효과검증
    1.문제정의 : 유저 체류 비율을 높이고 싶다
    2.가설설정 : 추천시스템으로 추천콘텐츠를 추천하여 유저 체류시간 늘림
    3.개입(개발) : 콘텐츠 추천시스템을 개발, 적용
    4.효과검증 : 평균 체류시간이 늘었는지 확인한다

    혹은 광고에 의해 매출이 올라갔는가?

* 모델 예측 정확도 != 비즈니스 수익률
  여기서 우리가 알고싶은것은 비즈니스 수익률
  (넷플릭스 추천시스템이 이뤄넨 사업적가치로 유저 해지율 저하를 사용 : 1년에 약 10억달러의 효과 있었음 )

* 오염된 로그:
  운영중인 모델이 사용자 행동 로그를 훈련데이터로 사용한다면
  사용자의 선택이 추천시스템에 영향을 주기때문에 편향될 수 있음 (추천한걸 선택하니까, 그 선택이 로그로 남고, 그걸 모델이 학습하니까 또 비슷한내용이 추천되는 순환)

<1. 가설 검정 hypothesis testing >
  표본(sample)을 통해 모집단에 유의한 차이가 발생했는지를 확인하는 기법
  샘플 표본의 속성을 추정해서 모집단의 성질을 알아보고자 하는 것

    - 귀무가설 null hypothesis:
    - 대립가설 alternative hypothesis:
    - P-value :
    - 유의수준 significant level:

    ex)
    귀무가설: A가 발생할 확률이 50%다!
    p-value: 이 귀무가설이 참일 확률
    유의수준: 이 p-값이 5%보다 낮으면 귀무가설 기각
    대립가설: A가 발생할 확률이 50%가 아니다!


  * 카이제곱검정
  * 이항분포를 정규근사한다?

  * (y(ground truth) != y^) : 거짓 false
    (y^(예측결과) == 1) : positive

    EX)
    유의수준이 5%라면 실제로는 두 그룹에 차이가 없는데도 5%확률로 차이가 있다고 오판할 수 있음
    이런 잘못된 판단을 거짓양성 false positive라 한다
    반대로, 유의한 차이가 있는데 귀무가설을 기각하지 못한 경우를 거짓음성 false negative이라고 한다

    주의?
    검정을 반복하다보면 > 언젠가는 유의한 차이가 나오게됨(P-value가 0.05까지 내려감)
    좋은 결과를 위해 한 시행착오가 오히려 거짓양성 확률을 높인다???

  * 표본 크기 커질수록 > p-value값 들쭉날쭉하던 편차가 점점 사라짐(삼각수렴) > P-value값이 0에 가까워짐 > but 유의한 차이가 나타남???
    (표본 크기가 커져서, 추정값의 표준편차가 작아지기 때문에 발생하는 현상)

    이 차이가 비즈니스에 미치는 영향과는 별개

  * 다중 검정 multiple testing : 여러 가설을 동시에 검정
    다중검정에서 거짓양성 억제하는 방법
    - FWER family wise error rate : 잘못된 판단을 하나라도 할 수 있는 확률을 낮추는 방법(ex: 본페로니 교정 Bonferroni correction)
    - FDR false discovery rate : 잘몬된 판단의 비중을 낮추는 방법

<2. 인과효과 추정 >
  인과효과추정: 사건 Y가 X의 영향을 얼마나 받았는지, 모집단에 대한 효과를 추정하는 방법 ( X -> 얼마나 영향 -> Y )
  (비교: 가설검정은, 샘플 표본의 속성을 추정해서 모집단의 성질을 알아보고자 하는 것)

  * 루빈 인과모형 Rubin Causal Model(RCM)
    - 개입 cause :                어떤 효과를 만드는 "예상"원인
    - 결과변수 outcome :           원하는 "예상"변화
    - 실험군 treatment group :    원인 요소를 적용시킨 그룹
    - 대조군(통제군)control group : 아무 개입도 안한 그룹

    이 실험은 개인단위에선 한쪽 경우만 관측 가능
    "광고(원인)를 본 A고객이 상품을 구매(결과)했다"면
    이 A고객이 광고를 안보았을때 결과는 이제 관측할수 없음(이미 봤기때문에...)
    그럼 '이 고객이 광고를 안보았다면?' 이처럼 관측은 불가능하지만 잠재적으로 존재할수 있던 결과변수를 "잠재적 결과변수"라함

    사용자 | 개입여부 | Y0 | Y1    (Y0: 광고 안본사람의 구매여부, Y1: 광고 본사람의 구매여부) ("-"는 값 누락의미)
    ========================
    1    |  1     | -  |  1
    2    |  1     | -  |  0
    3    |  1     | -  |  1
    4    |  0     | 0  |  -
    5    |  0     | 1  |  -

    사용자1의 Y1 - Y0은 측정 불가능 (하나가 관측 불가니까)
    그러나 E(Y1 - Y0)은 가능 = E(Y1) - E(Y0)

    개인단위에선 측정 불가능, 집단단위에서 측정가능 == 표본에 대한 효과가 아니라, 모집단에 대한 효과를 측정 가능
    평균처치효과(ATE average treatment effect)라함

    - 선택편향 selection bias 문제 있음 : 이미 "실험군"이 어떤 공통된 특성A를 가지고 있어서 "개입"원인 때문에 효과가 생긴게 아니라 A특성때문에 효과가 일어난 것일수 있음
      ex: 처음부터 실험군이 모두 여자고, 대조군이 모두 남자라면, 구매의 원인이 광고가 아니라 성별 때문일수있음

      선택편향 문제 >> 무작위 대조시험(RCT randomized controlled trial) 으로 해결!
      * RCT : 표본을 무작위로 선별(어떤 공통된 특성을 갖지 않도록) 후 실험군에만 개입 적용 == "표본에 무작로 개입여부 결정"

      시계열 데이터의 경우(어떤 상품의 매출 히스토리)
      상품이 계절적인 영향을 받는 거라면
      광고 때문에 구매가 많아진건지, 시간이 지나 계절이 바뀌어 구매가 많아진건지
      그 결과의 요인에 영향을 알수가 없음
      이럴때 RCT 를 하면
      동일 기간에 두 그룹 비교가 가능(개입 요인 이외의 요인을 통제할 수 있음: 두 그룹모두 동일 기간에 관측하므로 계절의 영향은 똑같이 받으니까)


<3. A/B 테스트 >
  * 진행과정
  1. 두 그룹선정
  2. A/A테스트( 두그룹모두 A환경만 테스트하여 두집단이 서로 균질한 집단인지, 선택편향은 없는지 확인하기 )
  3. A/B테스트( 한쪽 그룹에만 개입: B환경 테스트 ), 테스트간 상호작용없도록 주의
  4. 결과 확인, 테스트 종료(테스트 시한을 미리 정하여 일정기간 후 종료하기) (평균이 신뢰구간을 크게 벗어나면 차이 있는것 /혹은/ 그렇지 않다면 효과 없는 것???)


=========================================
<Chapter 7 : 머신러닝 실무 프로젝트 >

  < Case1 영화 추천 시스템 만들기 >
  * 영화 추천 시스템을 만들기 위해선
    - 추천 알고리즘
    - 데이터 입수 방법
    - 평가 척도
    를 먼저 알아봐야함

    - 예측결과는 DB에 저장하고 > app에서 DB에서 결과값 가져오는 식으로 설계

  * 시스템 정의 : 추천시스템이란? 과거 유저 행동 정보 기반 -> 선호 아이템 제시

  * 시스템 응용분야 :
    - 개요 추천 (broad recommendation lists) : 이번주 인기 매출상품 통계 기반, 편집자 마음
    - 사용자 평가 (user comments and ratings) : 타유저의 평가 통계
    - 알림 서비스 (notification services) : 유저가 흥미 느낄 아이템 추천으로 재방문 유도
    - 연관 아이템 추천 (item-associated recommendations) : 검색 상품과 관련된 악세서리 상품 추천 (노트북 -> 노트북 파우치)
    - 개인화 (deep personalization) : 검색결과를 유저 개인 별로 다르게 만듬 (ex: 검색어 "기생충" : 아픈 유저라면 기생충약, 영화광 유저라면 영화 기생충 dvd)
    Applications of Data Mining to Electronic Commerce(Springer, 2001)출처

  * 데이터 설계 와 데이터 입수

    * 추천시스템을 위한 주요 input 데이터
      - 선호 데이터: 유저가 A를 얼마나 선호하는지
      - 검색 쿼리: "가성비"좋은 A, "저렴한" A, "고급스러운" A, "휴대용" A 등등
      - 비평: 상품 댓글, 상품 평가
      - 아이템 특징: 상품 설명(상품 세부사항)
      - 인구통계적 특징: 성별, 나이, 거주지역 등
      - 맥락적 특징: A 사용 날짜, 위치, 재고 현황 등등

    * 데이터 종류
      - 명시적 데이터(explicit data) : 유저에게 "직접" 선호도 질문하여 데이터 얻음
      - 묵시적 데이터(implicit data) : "간접"적으로, 상품 열람 기록 등으로 추정

              데이터 양 | 데이터 정확성 | 미평가,부정적평가 구별 | 사용자의 인지
              ====================================================
      명시적 ||  작음    | 정확       |  가능              |선호에 대한 데이터수집한다는걸 인지
      묵시적 ||  많음    | 부정확      |  불가능            |인지 못함

      - 묵시적 데이터 양: 데이터양이 많으면 통계적 방법을 사용하기에 유리함 (명시적 데이터를 얻기위해선 유저에게 보상이 필요함)
                  반면, 특정 상품을 잘못 클릭한 것일수도 있기 때문에 사용하기 전에 필터링 필요! (상품페이지 체류시간을 필터로 사용)

      - 묵시적 데이터 미평가 구별 : 열람하지 않은 것에 대해 평가를 않는 것인지, 아니면 부정적 평가를 한것인지 구분 불가능 (아직 보지않은 선호하는 영화를 선호하지 않는 영화로 판정할 위험 있음)

      - 명시적 데이터 사용자의 인지: 시스템에서 선호도 데이터를 수집한다는 걸 유저가 인지함으로써, 근거있는 추천을 한다고 좋은 인상 줄수있음

      - 데이터가 희소(sparse)하면 프로젝트 어려워짐 > 이럴땐 간접지표(묵시적 데이터)를 사용함으로써 해결
      ex)어떤 영화는 흥행해서 많은 유저가 봤기 때문에 데이터가 많지만, 어떤 영화는 본 사람이 없어서 데이터도 없음
        해결법
        1. 명시적 데이터를 얻는 방법을 고민: 데이터 얻는 비용을 낮출방법 찾기
          (ex: 음악은 3분정도만 들으면 되기때문에 유저가 경험하고 평가하는데 비용이 작음)
        2. 묵시적 데이터를 얻는 방법을 고민: 어떤 데이터가 이것을 암시하는지 생각해보기
          (ex: 집구매나 예식장예약 같이 일생에 몇번 없는 이벤트는 유저가 그걸 다 경험하고 평가하는데 비용 큼
          그러므로 예식장 웹사이트 페이지 열람 로그를 지표로 사용하기)


    * 추천시스템 알고리즘
      크게 두가지
      - 협업 필터링(collaborative filtering) : 영화 취향이 비슷한 사람에게 추천 영화 묻기 (경향이 비슷한 다른 유저 정보 기반)
      - 내용 기반 필터링(content-based filtering) : 영화감독, 장르, 제목 등 (영화 자체의 정보를 기반)

      협업 필터링은 데이터가 충분히 쌓이기 전까지는 추천 어려움(콜드 스타트 문제!), 그러나 도메인 지식 필요 적음
      내용 기반 필터링은 누적 데이터가 없어도 적절한 추천 가능, 그러나 도메인 지식 필요

      * 사용자기반 협업필터링 과정
        1. 사용자 정보를 벡터화
        2. 사용자 간의 유사도 평가
        3. 유사도를 기반으로 평점을 계산

      * 유사도: 두 대상이 비슷할수록 값이 커지고, 다를수록 작아지는 척도
        대표적인 유사도
        - 피어슨 상관계수 Pearson product- moment correlation coefficient
        - 코사인 유사도 cosine similarity
        - 자카드 계수 Jaccard index

    * 평가 척도
      - 정확도: 예측결과와 사용자 평점을 비교
      - 정밀도: 예측결과 중 정답의 비율
      - 재현율: 전체 정답 중 실제 맞춘 정답의 비율
      - (MAE) mean absolute error: 회귀에서 예측값과 실제값의 오차
      - (RMSE) root mean squared error : MAE보다 이상값(outlier)에 취약하다는 단점,
      - 순위 상관 rank correlation : 추천한 아이템의 순서를 평가 (랭킹 학습 learn to rank 에 사용)
      - 다양성 diversity: 전체 아이템 중 평점을 예측할 수 있는 아이템의 비율 커버리지

  < 영화 추천 시스템 practice >
  * 데이터 일부 대강 살펴보기, 어떤 컬럼(피처)이 있는지
    - 사용자 정보 : 아이디, 나이, 성별, 우편번호 등
    - 영화 정보 : 영화제목, 개봉일 등

  * 평점 평균 내보기: 평점 수가 적은건 1명만 평점을 만점을 줘도 평균평점이 만점이 나옴!
    (특정 영화의 평점 수가 적으면 노이즈가 들어감)
    > 평점 개수가 100건 이상인 영화만 추려내기

  * 히스토그램으로 분포 보기 : 롱테일 분포(지프의 법칙 Zip's law) 보임을 파악

  * 평점 데이터의 통계 정보 보기: df['평점'].describe()
    > 사용자별 평가 횟수와 사용자별 평점 평균을 보면
    어떤유저는 평점을 매우 짜게줌 (유저들의 평점 평균 중 min값: 1.49인 유저)
    어떤유저는 평점을 후하게 줌 (유저들의 평점 평균 중 max값: 4.87인 유저)
    > 평점 표준화 필요 standardization

  * feature vector 만들기
    한 유저에 대한: 사용자 정보인 나이, 성별, 우편번호등을 일렬로 + 영화 정보인 장르, 감독 등을 일렬로 + 영화 평점 데이터를 일렬로
    >> (범주형 피쳐는 one-hot incoding )

  * 알고리즘에 학습, 평가

    인풋 피처벡터를 다양한 종류로 만들기
    ex: -1번- 나이 성별 장르 감독
        -2번- 성별 기간 감독
        -3번- ...
         이런식으로

         어떤 인풋데이터를 활용했을때 가장 RMSE가 낮은가 체크하기


  < Case2 킥스타터 분석 - 머신러닝을 사용하지 않는 선택지 >
  킥스타터 : 어떤 프로젝트에 후원하는 서비스?

  * 크롤러로 데이터 받기
  * JSON 데이터를 CSV로 변환
  * 엑셀로 데이터 훑어보기! : Raw data를 직접 보는 것은 분석 효율에 큰 영향을 줌. 이걸 안하면 분석 결과에 이상이 있어도 눈치채기 어려움
  * 모금액 달성률 기준으로 내림차순 그래프 그려보기 > 달성률 100% 언저리에 왠지 모를 특이점이 보임(그래프 곡선이 그부분만 갑자기 튀어나와있음)
  * "상태"가 "진행중"인 데이터로만 다시 동일 그래프 그려보기 > 특이점이 나타나지 않음
  * 그렇다면 이 특이점은 프로젝트 완료 직전에 나타난다는 것을 알 수 있다:
    예상 이유
    - 완료가 임박한 프로젝트를 킥스타터가 페이지 최상위에 배치하기 때문이다.
    - 완료가 임박하여 후원이 늘어나느 것은 설명이 되지만, 100% 언저리에 특이점이 생기는 이유는 설명이 되지 않느다.
    - 프로젝트가 완료될 즈음에 프로젝트 제안자가 더 열심히 홍보하기 때문이다.
    - 제안자의 지인들이 마지막에 후원에 동참하기 때문이다.
    - 달성이 확실한 프로젝트에만 참여하려는 사람이 후원에 참여하기 때문이다.
    - 자신의 참여로 프로젝트가 달성되는 것을 즐기는 사람이 있기 때문이다.

  * 피벗테이블로 데이터 분석하기(여러가지를 기준으로 시각화 해보기)
    -가로축에 일인당 후원액
    세로축에 달성률에 따른 건수, 건수 비율, 평균 후원액, 평균 후원 건수를 놓음
    -달성률과 연도 축을 비교

    - 시각화 > 특이점 발견 > 그것에대한 심화 분석

  * 보고서 작성

  * 킥스타터처럼 불연속적인 보상이 걸려있는 유형의 문제에서는 이 불연속이 생기는 구간을 집중 분석하는 방법이 효과적.
  현실세게에서도 이러한 왜곡이 많이 발생하므로, 데이터에서 어떤 왜곡을 관찰했을 때 불연속이 발생하는 위치와 그 위치 부근에서 어떤 현상이 일어나는지 살펴보면 재미있는 사실을 발견 가능
  킥스타터처럼 불연속적 보상 체계가 긍정적으로 작용하는 경우도 있지만,
  소득세 세율 구간처럼 부정적으로 작용하는 경우도 있다. 이런 불연속이 존재하는 주변을 분석하면 사업상의 큰 무기가 될 수 있음!
  
